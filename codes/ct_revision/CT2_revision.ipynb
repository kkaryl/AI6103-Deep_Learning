{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import utils\n",
    "import time\n",
    "\n",
    "import math\n",
    "import functools\n",
    "import operator\n",
    "from IPython.display import Math\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6653200 parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "#print(f'{sum(p.numel() for p in net.parameters() if p.requires_grad)} parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo:\n",
    "+ Sentiment Analysis using RNN?\n",
    "+ Stack RNN\n",
    "+ GRU\n",
    "+ Speech Recognition (Spectogram)\n",
    "+ MT \n",
    "+ Bidirectional LSTM\n",
    "+ ConvNet for NLP\n",
    "+ Attention CNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. CNN (labs_lecture08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Conv2D Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_conv_output(N, F, P=0, S=1):\n",
    "    return int(math.floor((N+2*P-F)/S + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_conv2d(conv2d, X):\n",
    "    # Here (1, 1) indicates that the batch size and the number of channels\n",
    "    # are both 1\n",
    "    #X = X.reshape((bs, nc) + X.shape)\n",
    "    Y = conv2d(X)\n",
    "    # Exclude the first two dimensions that do not interest us: examples and\n",
    "    # channels\n",
    "    return Y.shape[2:]\n",
    "    #return Y.reshape(Y.shape[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter size: torch.Size([5, 1, 3, 3])\n",
      "output size: torch.Size([1, 5, 6, 6])\n",
      "computed size: 6\n",
      "computed size: torch.Size([6, 6])\n"
     ]
    }
   ],
   "source": [
    "bs, nc = 1, 1\n",
    "x = torch.rand(bs, nc, 6,6)\n",
    "conv2d1 = nn.Conv2d(in_channels=nc, out_channels=5, kernel_size=3, padding=1)\n",
    "print(f\"filter size: {conv2d1.weight.size()}\") # filter size\n",
    "y = conv2d1(x); print(f\"output size: {y.size()}\")\n",
    "print(f\"computed size: {calc_conv_output(6, 3, P=1, S=1)}\")\n",
    "print(f\"computed size: {comp_conv2d(conv2d1, x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padding(N, F, S=1):\n",
    "    \"\"\"Computes same padding\"\"\"\n",
    "    P = ((S-1)*N-S+F)/2\n",
    "    print(f\"pad: {P}\")\n",
    "    return int(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad: 1.0\n",
      "padding: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"padding: {get_padding(6, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computed size: torch.Size([6, 8])\n"
     ]
    }
   ],
   "source": [
    "conv2d2 = nn.Conv2d(1, 1, kernel_size=(5, 3), padding=(2, 2))\n",
    "print(f\"computed size: {comp_conv2d(conv2d2, x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.4252, 0.5256, 0.9016, 0.9738, 0.1749, 0.5098],\n",
      "          [0.2569, 0.6369, 0.2054, 0.8163, 0.5875, 0.4681],\n",
      "          [0.5969, 0.7077, 0.5774, 0.9721, 0.2460, 0.7573],\n",
      "          [0.4752, 0.7360, 0.3614, 0.5428, 0.3666, 0.6142],\n",
      "          [0.9727, 0.7532, 0.5712, 0.7669, 0.3450, 0.8892],\n",
      "          [0.3048, 0.1747, 0.9293, 0.4978, 0.7745, 0.3060]],\n",
      "\n",
      "         [[0.1194, 0.5136, 0.4161, 0.9292, 0.1764, 0.9836],\n",
      "          [0.2437, 0.3880, 0.8240, 0.6900, 0.8286, 0.4023],\n",
      "          [0.8131, 0.5747, 0.4015, 0.3696, 0.2569, 0.1663],\n",
      "          [0.5540, 0.5938, 0.7130, 0.4867, 0.0861, 0.6473],\n",
      "          [0.0166, 0.8278, 0.2405, 0.0504, 0.5603, 0.6847],\n",
      "          [0.6472, 0.2229, 0.2747, 0.8148, 0.6128, 0.5177]]]])\n",
      "torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[0.6369, 0.9738, 0.5875],\n",
      "          [0.7360, 0.9721, 0.7573],\n",
      "          [0.9727, 0.9293, 0.8892]],\n",
      "\n",
      "         [[0.5136, 0.9292, 0.9836],\n",
      "          [0.8131, 0.7130, 0.6473],\n",
      "          [0.8278, 0.8148, 0.6847]]]])\n",
      "torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[0.4611, 0.7243, 0.4351],\n",
      "          [0.6289, 0.6134, 0.4960],\n",
      "          [0.5514, 0.6913, 0.5787]],\n",
      "\n",
      "         [[0.3162, 0.7148, 0.5977],\n",
      "          [0.6339, 0.4927, 0.2892],\n",
      "          [0.4286, 0.3451, 0.5939]]]])\n",
      "torch.Size([1, 2, 2, 2])\n",
      "tensor([[[[0.9016, 0.9738],\n",
      "          [0.9727, 0.8892]],\n",
      "\n",
      "         [[0.8240, 0.9836],\n",
      "          [0.8278, 0.8148]]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1,2,6,6)\n",
    "max_pool = nn.MaxPool2d(2,2)\n",
    "avg_pool = nn.AvgPool2d(2,2)\n",
    "adp_max_pool = nn.AdaptiveMaxPool2d((2,2))\n",
    "\n",
    "print(x)\n",
    "after_mp = max_pool(x); print(after_mp.size())\n",
    "print(after_mp)\n",
    "after_ap = avg_pool(x); print(after_ap.size())\n",
    "print(after_ap)\n",
    "after_amp = adp_max_pool(x); print(after_amp.size())\n",
    "print(after_amp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 LeNet5 Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "from utils import check_mnist_dataset_exists\n",
    "data_path=check_mnist_dataset_exists()\n",
    "\n",
    "train_data=torch.load(data_path+'mnist/train_data.pt')\n",
    "train_label=torch.load(data_path+'mnist/train_label.pt')\n",
    "test_data=torch.load(data_path+'mnist/test_data.pt')\n",
    "test_label=torch.load(data_path+'mnist/test_label.pt')\n",
    "\n",
    "print(train_data.size())\n",
    "print(test_data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data mean: 0.13066235184669495, std: 0.30810782313346863\n"
     ]
    }
   ],
   "source": [
    "def compute_train_data_statistics(train_data, device=device):\n",
    "    mean= train_data.mean().to(device)\n",
    "    std = train_data.std().to(device)\n",
    "    return mean, std\n",
    "\n",
    "mean, std = compute_train_data_statistics(train_data)\n",
    "print(f\"Train data mean: {mean}, std: {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import operator\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, input_dims=(1,28,28)):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.CL1 = nn.Conv2d(in_channels=1, out_channels=50, kernel_size=3, padding=get_padding(50, 3))\n",
    "        # 50 x 28 x 28\n",
    "        self.MP1 = nn.MaxPool2d(2,2)\n",
    "        # 50 x 14 x 14\n",
    "        self.CL2 = nn.Conv2d(in_channels=50, out_channels=100, kernel_size=3, padding=get_padding(100, 3))\n",
    "        # 100 x 14 x 14\n",
    "        self.MP2 = nn.MaxPool2d(2,2)\n",
    "        # 100 x 7 x 7 = 4900\n",
    "        \n",
    "        features = nn.Sequential(self.CL1, self.MP1, self.CL2, self.MP2)\n",
    "        num_feats_aft_conv = functools.reduce(operator.mul, list(features(torch.rand(1, *(input_dims))).shape)); #print(num_feats_aft_conv)\n",
    "        \n",
    "        self.LL1 = nn.Linear(num_feats_aft_conv, 100) #4900\n",
    "        self.LL2 = nn.Linear(100, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.CL1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.MP1(x)\n",
    "        \n",
    "        x = self.CL2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.MP2(x)\n",
    "        \n",
    "        #x1 = x.view(-1, 4900); print(x1.shape)\n",
    "        x = torch.flatten(x, 1)#; print(x.shape)\n",
    "        x = self.LL1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.LL2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad: 1.0\n",
      "pad: 1.0\n",
      "LeNet5(\n",
      "  (CL1): Conv2d(1, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (MP1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (CL2): Conv2d(50, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (MP2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (LL1): Linear(in_features=4900, out_features=100, bias=True)\n",
      "  (LL2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "There are 536710 (0.54 million) parameters in this neural network\n"
     ]
    }
   ],
   "source": [
    "net = LeNet5().to(device)\n",
    "print(net)\n",
    "utils.display_num_param(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_test_set():\n",
    "    running_error = 0\n",
    "    num_batches = 0\n",
    "    for i in range(0, len(test_data), bs):\n",
    "        #set_trace()\n",
    "        minibatch_data = test_data[i:i+bs].to(device)\n",
    "        minibatch_label = test_label[i:i+bs].to(device)\n",
    "        inputs = minibatch_data.unsqueeze(dim=1) # add batch dim (color no need)\n",
    "        inputs = (inputs - mean)/std   # normalize inputs\n",
    "        scores = net(inputs)\n",
    "        error = utils.get_error(scores, minibatch_label)\n",
    "        running_error += error.detach().item()\n",
    "        num_batches += 1\n",
    "    total_error = running_error / num_batches\n",
    "    print(f\"=> test error: {total_error*100:.5f}% \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "bs = 128\n",
    "lr = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1, time=0.07462 min, loss=0.26874, train error=8.60874%, lr=0.25000\n",
      "=> test error: 1.62184% \n",
      "\n",
      "epoch=2, time=0.12309 min, loss=0.04827, train error=1.51142%, lr=0.25000\n",
      "=> test error: 1.24604% \n",
      "\n",
      "epoch=3, time=0.17206 min, loss=0.03330, train error=1.01279%, lr=0.25000\n",
      "=> test error: 1.11748% \n",
      "\n",
      "epoch=4, time=0.22381 min, loss=0.02494, train error=0.82789%, lr=0.25000\n",
      "=> test error: 0.94937% \n",
      "\n",
      "epoch=5, time=0.27419 min, loss=0.01423, train error=0.40367%, lr=0.12500\n",
      "=> test error: 0.98892% \n",
      "\n",
      "epoch=6, time=0.32437 min, loss=0.01092, train error=0.30817%, lr=0.12500\n",
      "=> test error: 0.83070% \n",
      "\n",
      "epoch=7, time=0.37669 min, loss=0.00870, train error=0.20989%, lr=0.12500\n",
      "=> test error: 0.85047% \n",
      "\n",
      "epoch=8, time=0.42883 min, loss=0.00719, train error=0.18157%, lr=0.12500\n",
      "=> test error: 0.75158% \n",
      "\n",
      "epoch=9, time=0.48514 min, loss=0.00592, train error=0.15492%, lr=0.12500\n",
      "=> test error: 0.76147% \n",
      "\n",
      "epoch=10, time=0.54109 min, loss=0.00381, train error=0.06497%, lr=0.06250\n",
      "=> test error: 0.76147% \n",
      "\n",
      "epoch=11, time=0.59756 min, loss=0.00320, train error=0.06219%, lr=0.06250\n",
      "=> test error: 0.84059% \n",
      "\n",
      "epoch=12, time=0.65454 min, loss=0.00287, train error=0.05664%, lr=0.06250\n",
      "=> test error: 0.79114% \n",
      "\n",
      "epoch=13, time=0.71064 min, loss=0.00252, train error=0.03998%, lr=0.06250\n",
      "=> test error: 0.75158% \n",
      "\n",
      "epoch=14, time=0.76903 min, loss=0.00222, train error=0.03332%, lr=0.06250\n",
      "=> test error: 0.84059% \n",
      "\n",
      "epoch=15, time=0.82846 min, loss=0.00179, train error=0.02166%, lr=0.03125\n",
      "=> test error: 0.81092% \n",
      "\n",
      "epoch=16, time=0.88701 min, loss=0.00169, train error=0.01999%, lr=0.03125\n",
      "=> test error: 0.78125% \n",
      "\n",
      "epoch=17, time=0.94623 min, loss=0.00158, train error=0.01499%, lr=0.03125\n",
      "=> test error: 0.81092% \n",
      "\n",
      "epoch=18, time=1.00354 min, loss=0.00152, train error=0.01666%, lr=0.03125\n",
      "=> test error: 0.76147% \n",
      "\n",
      "epoch=19, time=1.06122 min, loss=0.00143, train error=0.01666%, lr=0.03125\n",
      "=> test error: 0.77136% \n",
      "\n",
      "epoch=20, time=1.11719 min, loss=0.00130, train error=0.01166%, lr=0.01562\n",
      "=> test error: 0.79114% \n",
      "\n",
      "epoch=21, time=1.17523 min, loss=0.00125, train error=0.00666%, lr=0.01562\n",
      "=> test error: 0.77136% \n",
      "\n",
      "epoch=22, time=1.23274 min, loss=0.00123, train error=0.00666%, lr=0.01562\n",
      "=> test error: 0.78125% \n",
      "\n",
      "epoch=23, time=1.29104 min, loss=0.00119, train error=0.00833%, lr=0.01562\n",
      "=> test error: 0.76147% \n",
      "\n",
      "epoch=24, time=1.34722 min, loss=0.00117, train error=0.00666%, lr=0.01562\n",
      "=> test error: 0.76147% \n",
      "\n",
      "epoch=25, time=1.40519 min, loss=0.00112, train error=0.00333%, lr=0.00781\n",
      "=> test error: 0.79114% \n",
      "\n",
      "epoch=26, time=1.46319 min, loss=0.00110, train error=0.00500%, lr=0.00781\n",
      "=> test error: 0.78125% \n",
      "\n",
      "epoch=27, time=1.51777 min, loss=0.00109, train error=0.00500%, lr=0.00781\n",
      "=> test error: 0.78125% \n",
      "\n",
      "epoch=28, time=1.57012 min, loss=0.00107, train error=0.00333%, lr=0.00781\n",
      "=> test error: 0.78125% \n",
      "\n",
      "epoch=29, time=1.62160 min, loss=0.00106, train error=0.00333%, lr=0.00781\n",
      "=> test error: 0.78125% \n",
      "\n",
      "epoch=30, time=1.67374 min, loss=0.00104, train error=0.00333%, lr=0.00391\n",
      "=> test error: 0.78125% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for epoch in range(1,31):\n",
    "    # update learning rate\n",
    "    if epoch % 5 == 0:\n",
    "        lr = lr / 2\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "        \n",
    "    running_loss=0\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "    \n",
    "    shuffled_indices=torch.randperm(len(train_data))\n",
    " \n",
    "    for count in range(0,len(train_data),bs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        indices=shuffled_indices[count:count+bs]         # [128]\n",
    "        minibatch_data = train_data[indices].to(device)  # [128, 28, 28]\n",
    "        minibatch_label= train_label[indices].to(device) # [128]\n",
    "        \n",
    "        # add batch dimension (color no need)\n",
    "        inputs = minibatch_data.unsqueeze(dim=1)         # [128, 1, 28, 28]\n",
    "        inputs = (inputs - mean)/std # normalize inputs\n",
    "        \n",
    "        inputs.requires_grad_()\n",
    "\n",
    "        scores= net(inputs)                              # [128, 10]\n",
    "        loss =  criterion(scores, minibatch_label) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # COMPUTE STATS\n",
    "        running_loss += loss.detach().item()\n",
    "        error = utils.get_error(scores.detach(), minibatch_label)\n",
    "        running_error += error.detach().item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    total_loss = running_loss/num_batches\n",
    "    total_error = running_error/num_batches\n",
    "    elapsed_time = (time.time()-start)/60\n",
    "    \n",
    "    print(f\"epoch={epoch}, time={elapsed_time:.5f} min, loss={total_loss:.5f}, train error={total_error*100:.5f}%, lr={lr:.5f}\")\n",
    "    eval_on_test_set() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# choose a picture at random\n",
    "idx = randint(0, len(test_data)-1)\n",
    "im = test_data[idx]\n",
    "\n",
    "# diplay the picture\n",
    "utils.show(im)\n",
    "\n",
    "# send to device, rescale, and view as a batch of 1 \n",
    "im = im.to(device)\n",
    "im= (im-mean) / std\n",
    "im= im.view(1,28,28).unsqueeze(dim=1) #cifar im.view(1,3,32,32)\n",
    "\n",
    "# feed it to the net and display the confidence scores\n",
    "scores =  net(im) \n",
    "probs= F.softmax(scores, dim=1)\n",
    "utils.show_prob_mnist(probs.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. RNN (labs_lecture10)\n",
    "+ Perplexity = exp(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Load Penn Tree Bank Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import check_ptb_dataset_exists\n",
    "data_path = check_ptb_dataset_exists('../data/'); print(data_path)\n",
    "\n",
    "train_data  =  torch.load(data_path+'ptb/train_data.pt')\n",
    "test_data   =  torch.load(data_path+'ptb/test_data.pt')\n",
    "\n",
    "print(train_data.size()) # 20 batches (sub-documents) of 46k words\n",
    "print(test_data.size())\n",
    "print(f\"sanity check: ({len(train_data)}+{len(test_data)})x20 = {(len(train_data)+len(test_data))*20} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Three Layer VRNN\n",
    "+ 1 hidden state represented by vector h of embedding size (e.g. 150)\n",
    "+ $h_t = tanh( Rh_{t-1} + Vg_{t} )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "hidden_size = 150 # Embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class three_layer_rnn(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.layer2 = nn.RNN(hidden_size, hidden_size)\n",
    "        self.layer3 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, word_seq, h_init):\n",
    "        # word_seq = [sent_len, batch_size]\n",
    "        g_seq = self.layer1(word_seq)             # [35, 20] -> [35, 20, 150]\n",
    "        # g_seq (embedding) = [sent_len, batch_size, emb dim]\n",
    "        h_seq, h_out = self.layer2(g_seq, h_init) # seq: [35, 20, 150], h: [1, 20, 150]\n",
    "        # h_seq = [sent_len, batch_size, emb dim]\n",
    "        # h_out = [1, batch_size, emb dim]\n",
    "        score_seq = self.layer3(h_seq)            # [1, 20, 150]\n",
    "        \n",
    "        return score_seq, h_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = three_layer_rnn(vocab_size, hidden_size).to(device)\n",
    "print(net)\n",
    "utils.display_num_param(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Initialize Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net.layer1.weight.data) # embedding\n",
    "print(net.layer3.weight.data) # linear\n",
    "\n",
    "# initialize weights of Embedding and Linear layers\n",
    "net.layer1.weight.data.uniform_(-0.1, 0.1)\n",
    "net.layer3.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "print(net.layer1.weight.data) # embedding\n",
    "print(net.layer3.weight.data) # linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 Training three-layer RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "my_lr = 1\n",
    "bs = 20 # data already prepared in batches of 20.\n",
    "seq_length = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_test_set():\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "       \n",
    "    h = torch.zeros(1, bs, hidden_size).to(device) #[1, 20, 150]\n",
    "\n",
    "    for count in range(0, len(test_data)-seq_length, seq_length) :  \n",
    "        minibatch_data =  test_data[count   : count+seq_length  ].to(device) #[35, 20]\n",
    "        minibatch_label = test_data[count+1 : count+seq_length+1].to(device) #[35, 20] shift by one word\n",
    "                                  \n",
    "        scores, h = net(minibatch_data, h) # scores: [35, 20, 10000]\n",
    "        \n",
    "        scores          =  scores.view(bs*seq_length, vocab_size) #[700, 10000]\n",
    "        minibatch_label =  minibatch_label.view(bs*seq_length)    #[700]  \n",
    "        \n",
    "        loss = criterion(scores, minibatch_label)    \n",
    "        \n",
    "        h = h.detach()\n",
    "            \n",
    "        running_loss += loss.detach().item()\n",
    "        num_batches += 1        \n",
    "    \n",
    "    total_loss = running_loss/num_batches \n",
    "    print(f\"=> test exp(loss): {math.exp(total_loss):.5f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "\n",
    "for epoch in range(10):\n",
    "    # keep the learning rate to 1 during the first 4 epochs, then divide by 1.1 at every epoch\n",
    "    if epoch >= 4:\n",
    "        my_lr = my_lr / 1.1\n",
    "    # create a new optimizer and give the current learning rate.   \n",
    "    optimizer=torch.optim.SGD(net.parameters(), lr=my_lr)\n",
    "        \n",
    "    # set the running quantities to zero at the beginning of the epoch\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "    # set the initial h to be the zero vector\n",
    "    h = torch.zeros(1, bs, hidden_size).to(device) #one word, bs documents, embedding size\n",
    "\n",
    "    for count in range(0, len(train_data)-seq_length, seq_length):\n",
    "        \n",
    "        # Set the gradients to zeros\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # create a minibatch\n",
    "        minibatch_data =  train_data[count   : count+seq_length  ].to(device) # [35, 20]\n",
    "        minibatch_label = train_data[count+1 : count+seq_length+1].to(device) # [35, 20]       \n",
    "        \n",
    "        # Detach to Stop backpropagation to go beyond the current mini-batch\n",
    "        # Then tell Pytorch to start tracking all operations that will be done on h\n",
    "        h = h.detach()\n",
    "        h = h.requires_grad_()\n",
    "                       \n",
    "        # forward the minibatch through the net        \n",
    "        scores, h = net(minibatch_data, h)                                    # [35, 20, 10000]\n",
    "        \n",
    "        # reshape the scores and labels to huge batch of size bs*seq_length\n",
    "        scores          = scores.view(bs*seq_length, vocab_size)              # [700, 10000]\n",
    "        minibatch_label = minibatch_label.view(bs*seq_length)                 # [700]\n",
    "        \n",
    "        # Compute the average of the losses of the data points in this huge batch\n",
    "        loss = criterion(scores, minibatch_label)\n",
    "        \n",
    "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
    "        utils.normalize_gradient(net) # prevent exploding gradients in RNNs.\n",
    "        optimizer.step()              # update weights\n",
    "\n",
    "        # update the running loss  \n",
    "        running_loss += loss.detach().item()\n",
    "        num_batches += 1\n",
    "        \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    elapsed = time.time()-start/60\n",
    "    \n",
    "    print(f\"epoch={epoch}, time={elapsed:.5f} min, exp(loss)={math.exp(total_loss):.5f} lr={my_lr:.5f}\")\n",
    "    eval_on_test_set() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 Testing three-layer RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"some analysts expect oil prices to remain relatively\"\n",
    "sentence2 = \"over the next days and weeks they say investors should look for stocks to\"\n",
    "sentence3 = \"prices averaging roughly $ N a barrel higher in the third\"\n",
    "sentence4 = \"i think my line has been very consistent mrs. hills said at a news\"\n",
    "sentence5 = \"this appears particularly true at gm which had strong sales in\"\n",
    "# or make your own sentence.  No capital letter or punctuation allowed. Each word must be in the allowed vocabulary.\n",
    "sentence6= \"he was very\"\n",
    "mysentence = sentence6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sen=utils.sentence2vector(mysentence).to(device)\n",
    "print(minibatch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.zeros(1, 1, hidden_size).to(device) # one word, one document\n",
    "scores, h = net(test_sen, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysentence, '... \\n')\n",
    "utils.show_next_word(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Long Short-Term Memory (LSTM)\n",
    "+ 2 hidden states represented by 2 vectors h and c of embedding size (e.g. 200)\n",
    "    + Vector h stores the short-term dependencies\n",
    "    + Vector c records the long-term dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 20\n",
    "vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class three_layer_lstm(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.layer2 = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.layer3 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, word_seq, h_init, c_init):\n",
    "        g_seq                 = self.layer1(word_seq)\n",
    "        h_seq, (h_out, c_out) = self.layer2(g_seq, (h_init, c_init))\n",
    "        score_seq             = self.layer3(h_seq)\n",
    "        return score_seq, h_out, c_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "net = three_layer_lstm(vocab_size, hidden_size).to(device)\n",
    "print(net)\n",
    "utils.display_num_param(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 Initialize Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights of Embedding and Linear layers\n",
    "net.layer1.weight.data.uniform_(-0.1, 0.1)\n",
    "net.layer3.weight.data.uniform_(-0.1, 0.1)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2 Training three-layer LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "my_lr = 5\n",
    "seq_length = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_test_set():\n",
    "    running_loss = 0\n",
    "    num_batches = 0\n",
    "    h = torch.zeros(1, bs, hidden_size).to(device)\n",
    "    c = torch.zeros(1, bs, hidden_size).to(device)\n",
    "    for count in range(0, len(test_data)-seq_length, seq_length):\n",
    "        minibatch_data =  test_data [count  : count+seq_length  ].to(device)\n",
    "        minibatch_label = test_data[count+1: count+seq_length+1].to(device)\n",
    "        \n",
    "        scores, h, c = net(minibatch_data, h, c)\n",
    "        \n",
    "        scores          = scores.view(bs*seq_length, vocab_size)\n",
    "        minibatch_label = minibatch_label.view(bs*seq_length)\n",
    "        \n",
    "        loss = criterion(scores, minibatch_label)\n",
    "        \n",
    "        h = h.detach()\n",
    "        c = c.detach()\n",
    "        \n",
    "        running_loss += loss.detach().item()\n",
    "        num_batches += 1\n",
    "    total_loss = running_loss / num_batches\n",
    "    print(f\"=> test exp(loss): {math.exp(total_loss):.5f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "\n",
    "for epoch in range(8):\n",
    "    # divide the learning rate by 3 except after the first epoch\n",
    "    if epoch >= 2:\n",
    "        my_lr = my_lr / 3\n",
    "    # create a new optimizer at the beginning of each epoch: give the current learning rate.   \n",
    "    optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
    "    \n",
    "    # set the running quatities to zero at the beginning of the epoch\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "\n",
    "    # set the initial h and c to be the zero vector\n",
    "    h = torch.zeros(1, bs, hidden_size).to(device)\n",
    "    c = torch.zeros(1, bs, hidden_size).to(device)\n",
    "    \n",
    "    for count in range(0 , len(train_data)-seq_length, seq_length):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # create a minibatch\n",
    "        minibatch_data =  train_data[count  :count+seq_length  ].to(device)\n",
    "        minibatch_label = train_data[count+1:count+seq_length+1].to(device)        \n",
    "        \n",
    "        h=h.detach()\n",
    "        c=c.detach()\n",
    "        h=h.requires_grad_()\n",
    "        c=c.requires_grad_()\n",
    "                       \n",
    "        # forward the minibatch through the net        \n",
    "        scores, h, c = net(minibatch_data, h, c)\n",
    "        \n",
    "        scores          =  scores.view(bs*seq_length, vocab_size)  \n",
    "        minibatch_label =  minibatch_label.view(bs*seq_length)       \n",
    "        loss = criterion(scores, minibatch_label)\n",
    "        loss.backward()\n",
    "\n",
    "        utils.normalize_gradient(net)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.detach().item()\n",
    "        num_batches += 1\n",
    "        \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    elapsed = (time.time()-start)/60\n",
    "    \n",
    "    print(f\"epoch={epoch}, time={elapsed:.5f} min, exp(loss)={math.exp(total_loss):.5f} lr={my_lr:.5f}\")\n",
    "    eval_on_test_set() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysentence = sentence6\n",
    "test_sen = utils.sentence2vector(mysentence).to(device)\n",
    "h = torch.zeros(1, 1, hidden_size).to(device)\n",
    "c = torch.zeros(1, 1, hidden_size).to(device)\n",
    "scores , h, c = net(test_sen , h, c)\n",
    "print(mysentence, '... \\n')\n",
    "utils.show_next_word(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 [Additional] Bidirectional LSTM (BiLSTM) WEIRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 20\n",
    "vocab_size = 10000\n",
    "hidden_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.layer2 = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers, bidirectional=True)\n",
    "        self.layer3 = nn.Linear(hidden_size*2, num_classes) # bidirectional\n",
    "        \n",
    "    def forward(self, word_seq, h_init, c_init):\n",
    "        g_seq                 = self.layer1(word_seq)\n",
    "        h_seq, (h_out, c_out) = self.layer2(g_seq, (h_init, c_init))\n",
    "        score_seq             = self.layer3(h_seq)\n",
    "        return score_seq, h_out, c_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BiLSTM(vocab_size, hidden_size, 1, vocab_size).to(device)\n",
    "print(net)\n",
    "utils.display_num_param(net)\n",
    "net.layer1.weight.data.uniform_(-0.1, 0.1)\n",
    "net.layer3.weight.data.uniform_(-0.1, 0.1)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "my_lr = 5\n",
    "seq_length = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_test_set():\n",
    "    running_loss = 0\n",
    "    num_batches = 0\n",
    "    h = torch.zeros(2, bs, hidden_size).to(device) # bidirectional\n",
    "    c = torch.zeros(2, bs, hidden_size).to(device) # bidirectional\n",
    "    for count in range(0, len(test_data)-seq_length, seq_length):\n",
    "        minibatch_data =  test_data [count  : count+seq_length  ].to(device)\n",
    "        minibatch_label = test_data[count+1: count+seq_length+1].to(device)\n",
    "        \n",
    "        scores, h, c = net(minibatch_data, h, c)\n",
    "        \n",
    "        scores          = scores.view(bs*seq_length, vocab_size)\n",
    "        minibatch_label = minibatch_label.view(bs*seq_length)\n",
    "        \n",
    "        loss = criterion(scores, minibatch_label)\n",
    "        \n",
    "        h = h.detach()\n",
    "        c = c.detach()\n",
    "        \n",
    "        running_loss += loss.detach().item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    set_trace()\n",
    "    total_loss = running_loss / num_batches\n",
    "    print(f\"=> test exp(loss): {math.exp(total_loss):.5f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "\n",
    "for epoch in range(8):\n",
    "    # divide the learning rate by 3 except after the first epoch\n",
    "    if epoch >= 2:\n",
    "        my_lr = my_lr / 3\n",
    "    # create a new optimizer at the beginning of each epoch: give the current learning rate.   \n",
    "    optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
    "    \n",
    "    # set the running quatities to zero at the beginning of the epoch\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "\n",
    "    # set the initial h and c to be the zero vector\n",
    "    h = torch.zeros(2, bs, hidden_size).to(device) # bidirectional\n",
    "    c = torch.zeros(2, bs, hidden_size).to(device) # bidirectional\n",
    "    \n",
    "    for count in range(0 , len(train_data)-seq_length, seq_length):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # create a minibatch\n",
    "        minibatch_data =  train_data[count  :count+seq_length  ].to(device)\n",
    "        minibatch_label = train_data[count+1:count+seq_length+1].to(device)        \n",
    "        \n",
    "        h=h.detach()\n",
    "        c=c.detach()\n",
    "        h=h.requires_grad_()\n",
    "        c=c.requires_grad_()\n",
    "                       \n",
    "        # forward the minibatch through the net\n",
    "        scores, h, c = net(minibatch_data, h, c)\n",
    "        \n",
    "        scores          =  scores.view(bs*seq_length, vocab_size)  \n",
    "        minibatch_label =  minibatch_label.view(bs*seq_length)       \n",
    "        loss = criterion(scores, minibatch_label)\n",
    "        loss.backward()\n",
    "\n",
    "        utils.normalize_gradient(net)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.detach().item()\n",
    "        num_batches += 1\n",
    "        \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    elapsed = (time.time()-start)/60\n",
    "    \n",
    "    print(f\"epoch={epoch}, time={elapsed:.5f} min, exp(loss)={math.exp(total_loss):.5f} lr={my_lr:.5f}\")\n",
    "    eval_on_test_set() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysentence = sentence6\n",
    "test_sen = utils.sentence2vector(mysentence).to(device)\n",
    "h = torch.zeros(2, 1, hidden_size).to(device)\n",
    "c = torch.zeros(2, 1, hidden_size).to(device)\n",
    "scores , h, c = net(test_sen , h, c)\n",
    "print(mysentence, '... \\n')\n",
    "utils.show_next_word(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
